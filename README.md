# Multilayer Perceptron Training for MNIST Classification

## Description
This project aim to train a MLP model for MNIST Classification. It's a example code in my presentation about MLP and Backpropagation. 

## Configuration
- Activation function: ReLU
- Input layer: 784 neurons
- 2 hidden layer: [521, 125] neurons
- Output layer: 10 neurons
- Stochastic gradient descent
- Learning Rate: 0.1
- Momentum: 0.9
## Test Accuracy: 91.02%

